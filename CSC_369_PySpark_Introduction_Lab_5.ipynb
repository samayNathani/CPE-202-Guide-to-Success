{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC 369 - PySpark Introduction - Lab 5.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samayNathani/CPE-202-Guide-to-Success/blob/main/CSC_369_PySpark_Introduction_Lab_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQ9QCIhsShu"
      },
      "source": [
        "# Introduction to Apache Spark / PySpark\n",
        "\n",
        "As of May 2021, these examples are based on [Spark 3.1.1](https://spark.apache.org/docs/3.1.1/)  \n",
        "\n",
        "Reference/API Links\n",
        "\n",
        "\n",
        "*   [Apache Spark Quick Start](https://spark.apache.org/docs/3.1.1/quick-start.html)\n",
        "*   [PySpark v3.1.1 API](https://spark.apache.org/docs/3.1.1/api/python/reference/index.html)\n",
        "*    [RDD Programming Guide](https://spark.apache.org/docs/3.1.1/rdd-programming-guide.html)\n",
        "*    [Spark SQL Programming Guide](https://spark.apache.org/docs/3.1.1/sql-programming-guide.html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoolY63aauhH",
        "outputId": "07c4153d-4f7d-441b-a7f7-89189131bead"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 61kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 14.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=3b7d70da45f0874517c1b1d85dadafcd8e55a99e88999b998b040dc6e1c4e556\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u292-b10-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u292-b10-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad7fmwTbMwW"
      },
      "source": [
        "\n",
        "# Imports / Starter Example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKUgYFX9olK5"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql import types as sparktypes\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "sc = SparkContext() \n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z53FFQEDayTj"
      },
      "source": [
        "# create a Resilient Distributed Dataset (RDD) from a sequence of integers perform filter() and reduce() operations\n",
        "\n",
        "# Function to be used in the filter() transformation\n",
        "def filterSmall(x):    \n",
        "   if x < 20 == 0:\n",
        "      return False\n",
        "   else:\n",
        "      return True\n",
        "\n",
        "# Function to be used in the map() transformation\n",
        "def mapSquare(x):\n",
        "    return x*x\n",
        "\n",
        "# Function to be used in the reduce() action\n",
        "def reduceSim(x,y):\n",
        "    return x+y\n",
        "  \n",
        "rdd = sc.parallelize(range(100))         ## create an RDD of 100 numbers from 0 to 99\n",
        "\n",
        "out1 = rdd.filter(filterSmall).map(mapSquare)  ## perform filter and map transformations\n",
        "out2 = out1.reduce(reduceSim)                  ## perform reduce operation\n",
        "\n",
        "print(out1.collect())           ## print first output (all numbers less than 20 squared)\n",
        "print(out2)                 ## print second output (sum of all numbers from first output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slRw9-jLsHOd"
      },
      "source": [
        "# download a sample access log for use in demos below\n",
        "!rm -f apache.access.log\n",
        "!wget -q https://raw.githubusercontent.com/databricks/reference-apps/master/logs_analyzer/data/apache.access.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmAntGt8iNli"
      },
      "source": [
        "# Apache HTTP Log Example - Resilient Distributed Dataset (RDD)\n",
        "\n",
        "A SparkContext instance can be used to create RDDs from various data/files/resources (text files, CSV, Hadoop data files, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUbUL01ff2j5"
      },
      "source": [
        "# find the top 10 clients, map/reduce style using RDD transformations\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .flatMap(lambda line: line.split(\" \")[0])  # field 0 = client address\n",
        "                  .map(lambda hn: (hn, 1) )\n",
        "                  .reduceByKey(lambda x, y: x + y)\n",
        "                  .sortBy(lambda t: -t[1])) \n",
        "\n",
        "print (\"Total count of client hostnames:\")\n",
        "print(access_log_rdd.count())\n",
        "\n",
        "print (\"Top 10 client hostnames:\")\n",
        "print(access_log_rdd.take(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ca1vYvspy6_"
      },
      "source": [
        "# Apache HTTP Log Example - DataFrame\n",
        "\n",
        "A DataFrame is equivalent to a relational table in Spark SQL, and can be created from on a variety of input formats (CSV, JSON, relational database, etc.) using the SparkSession."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wccwGjPmBBL"
      },
      "source": [
        "access_log_df = spark.read.text(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqc5g2nNqq3e"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "access_log_df.show(truncate=False)\n",
        "access_log_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuD2cLn6p_Ob"
      },
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('path'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))\n",
        "named_df.show(truncate=False)\n",
        "named_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdpYRf3z9y_h"
      },
      "source": [
        "named_df.createOrReplaceTempView(\"log\")\n",
        "sql_df = spark.sql(\"SELECT * FROM log WHERE status = 404\")\n",
        "\n",
        "sql_df.show(truncate=False)\n",
        "sql_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBrUNHSBtgv"
      },
      "source": [
        "## What About Datasets?\n",
        "\n",
        "Added in Spark 1.6, a **Dataset** is a distributed collection of data that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (`map`, `flatMap`, `filter`, etc.). The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). The case for R is similar.\n",
        "\n",
        "A **DataFrame** is a Dataset organized into named columns. ([source](https://spark.apache.org/docs/3.1.1/sql-programming-guide.html)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ2zn6ezgTmF"
      },
      "source": [
        "# Reporting Tasks (from Lab 1)\n",
        "\n",
        "\n",
        "1. Most popular URL paths (top 15)\n",
        "2. Request count for each HTTP response code, sorted by response code\n",
        "3. Request count for each calendar month and year, sorted chronologically\n",
        "4. Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "5. Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPvNuJGgkQl"
      },
      "source": [
        "# (A) RDD Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using RDD transformations\n",
        "\n",
        "[RDD APIs PySpark v3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html#rdd-apis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNTpq69g1pU"
      },
      "source": [
        "# RDD implementation\n",
        "# (1) Most popular URL paths (top 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az250Dgghvd5"
      },
      "source": [
        "# RDD implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa17VZSyhv4X"
      },
      "source": [
        "# RDD implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSUdQb4Mhv-W"
      },
      "source": [
        "# RDD implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi89G8EQhwEO"
      },
      "source": [
        "# RDD implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxkjTvxIhBO-"
      },
      "source": [
        "# (B) DataFrame Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark's DataFrame API\n",
        "\n",
        "[DataFrame API PySpark v.3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGYTdWLhJCH"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (1) Most popular URL paths (top 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apq4hUSVh94J"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSvjvCNth9-S"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjkoHWOxh-Df"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwpDYxTh-Iv"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ml-MwXXhbT6"
      },
      "source": [
        "# (C) Spark SQL Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark SQL\n",
        "\n",
        "[Spark SQL API PySpark v3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html)\n",
        "\n",
        "Specifically, [pyspark.sql.SparkSession.sql](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.sql.html#pyspark.sql.SparkSession.sql) returns a DataFrame representing the result of the given SQL query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kk-wMyahiZy"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (1) Most popular URL paths (top 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEHU_koHiFLH"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (2) Request count for each HTTP response code, sorted by response code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_DVj46HiFQY"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (3) Request count for each calendar month and year, sorted chronologically"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eOvUdlziFUP"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYxy0gSoiFYe"
      },
      "source": [
        "# Spark SQL implementation \n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}